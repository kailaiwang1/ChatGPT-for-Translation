{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import msal\n",
    "\n",
    "AZURE_AUTHORITY = \"https://login.microsoftonline.com/fcb2b37b-5da0-466b-9b83-0014b67a7c78\"\n",
    "AZURE_CLIENT_ID = \"fdbc25f6-c263-4d93-b5ce-640e8d35aee7\"\n",
    "AZURE_SCOPE = \"https://bayer.com/prod-baychatgpt-api/.default\"\n",
    "\n",
    "\n",
    "_msal_auth = msal.PublicClientApplication(\n",
    "        client_id=AZURE_CLIENT_ID,\n",
    "        authority=AZURE_AUTHORITY,\n",
    ")\n",
    "\n",
    "token_meta = _msal_auth.acquire_token_interactive(scopes=[AZURE_SCOPE])\n",
    "token = token_meta['access_token']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "import os\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "import requests\n",
    "# import trafilatura\n",
    "from tqdm import tqdm\n",
    "# from utils.bilingual_txt_to_docx import create_bilingual_docx\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "from concurrent.futures import as_completed\n",
    "\n",
    "# currenly on .txt file could run successfully\n",
    "ALLOWED_FILE_TYPES = [\n",
    "    \".txt\",\n",
    "    \".md\",\n",
    "    \".rtf\",\n",
    "    \".html\",\n",
    "    \".pdf\",\n",
    "]\n",
    "AZURE_API_VERSION = \"2023-03-15-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='gpt_4_8k_ascent'\n",
    "openai.api_base = \"https://chat.int.bayer.com/api/v1\"\n",
    "openai.api_key = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def translate(token, target_language, text, options=None):\n",
    "    # Set up OpenAI API auth\n",
    "    # openai.api_type = \"azure\"\n",
    "    # openai.api_version = AZURE_API_VERSION\n",
    "    openai.api_base = \"https://chat.int.bayer.com/api/v1\"\n",
    "    openai.api_key = token\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Set up the prompt\n",
    "    messages = [{\n",
    "        'role': 'system',\n",
    "        'content': 'You are a translator assistant.'\n",
    "    }, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\":\n",
    "            f\"Translate the following text into {target_language}. Retain the original format. Return only the translation and nothing else:\\n{text}\",\n",
    "    }]\n",
    "    \n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model='gpt_4_8k_ascent', #options.model\n",
    "        messages=messages\n",
    "        )\n",
    "\n",
    "    t_text = (completion[\"choices\"][0].get(\"message\").get(\n",
    "        \"content\").encode(\"utf8\").decode())\n",
    "\n",
    "    return t_text\n",
    "def remove_empty_paragraphs(text):\n",
    "    # Split the text into paragraphs\n",
    "    if isinstance(text, str):\n",
    "        text = text.split('\\n')\n",
    "\n",
    "    # Filter out empty paragraphs\n",
    "    non_empty_paragraphs = filter(lambda p: p.strip() != '', text)\n",
    "\n",
    "    # Join the non-empty paragraphs back into a string\n",
    "    return '\\n'.join(non_empty_paragraphs)\n",
    "def translate_text_file(text_filepath_or_url, options): #url should be disabled here\n",
    "    OPENAI_API_KEY = token #options.openai_key or os.environ.get(\"OPENAI_API_KEY\")\n",
    "    target_language = options['target_language'] #'English'\n",
    "\n",
    "    paragraphs = read_and_preprocess_data(text_filepath_or_url, options)\n",
    "\n",
    "    # Create a list to hold your translated_paragraphs. We'll populate it as futures complete.\n",
    "    translated_paragraphs = [None for _ in paragraphs]\n",
    "\n",
    "    # Submit your translation tasks\n",
    "    futures = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        for idx, text in enumerate(paragraphs):\n",
    "            future = executor.submit(\n",
    "                translate,\n",
    "                OPENAI_API_KEY,\n",
    "                target_language,\n",
    "                text,\n",
    "                options=options\n",
    "            )\n",
    "            futures.append((idx, future))\n",
    "        # Iterate over the futures as they complete.\n",
    "        for future in tqdm(as_completed([future for idx, future in futures]), total=len(paragraphs), desc=\"Translating paragraphs\", unit=\"paragraph\"):\n",
    "            for idx, f in futures:\n",
    "                if f == future:\n",
    "                    try:\n",
    "                        translated_paragraphs[idx] = future.result().strip()\n",
    "                    except Exception as e:\n",
    "                        print(f\"An error occurred during translation: {e}\")\n",
    "                        translated_paragraphs[idx] = \"\"  # or however you want to handle errors\n",
    "\n",
    "\n",
    "    translated_text = \"\\n\".join(translated_paragraphs)\n",
    "\n",
    "    # Output translated text file\n",
    "    # remove extra newlines\n",
    "    translated_text = re.sub(r\"\\n{2,}\", \"\\n\", translated_text)\n",
    "\n",
    "    translated_text = remove_empty_paragraphs(translated_text)\n",
    "    output_file_translated = f\"{Path(text_filepath_or_url).parent}/{Path(text_filepath_or_url).stem}_translated.txt\"\n",
    "    with open(output_file_translated, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(translated_text)\n",
    "        print(f\"Translated text saved to {f.name}.\")\n",
    "    \n",
    "\n",
    "\n",
    "# def download_html(url):\n",
    "#     response = requests.get(url)\n",
    "#     return response.text\n",
    "\n",
    "\n",
    "from utils.parse_pdfs.parse_tei_xml import extract_paper_info\n",
    "from pathlib import Path\n",
    "# import trafilatura\n",
    "\n",
    "def read_and_preprocess_data(text_filepath_or_url, options):\n",
    "    if text_filepath_or_url.startswith('http'):\n",
    "        pass\n",
    "        # # replace \"https:/www\" with \"https://www\"\n",
    "        # text_filepath_or_url = text_filepath_or_url.replace(\":/\", \"://\")\n",
    "        # # download and extract text from URL\n",
    "        # print(\"Downloading and extracting text from URL...\")\n",
    "        # downloaded = trafilatura.fetch_url(text_filepath_or_url)\n",
    "        # print(\"Downloaded text:\")\n",
    "        # print(downloaded)\n",
    "        # text = trafilatura.extract(downloaded)\n",
    "    elif text_filepath_or_url.endswith('.pdf'):\n",
    "        # extract text from PDF file\n",
    "        print(\"Extracting text from PDF file...\")\n",
    "        extract_paper_info(text_filepath_or_url)\n",
    "        # use newly created txt file\n",
    "        text_filepath_or_url = f\"{Path(text_filepath_or_url).parent}/{Path(text_filepath_or_url).stem}_extracted.txt\"\n",
    "        with open(text_filepath_or_url, \"r\", encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    else:\n",
    "        with open(text_filepath_or_url, \"r\", encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            if text_filepath_or_url.endswith('.html'):\n",
    "                # # extract text from HTML file\n",
    "                # print(\"Extracting text from HTML file...\")\n",
    "                # text = trafilatura.extract(text)\n",
    "                \n",
    "                # # write to a txt file ended with \"_extracted\"\n",
    "                # with open(\n",
    "                #         f\"{Path(text_filepath_or_url).parent}/{Path(text_filepath_or_url).stem}_extracted.txt\",\n",
    "                #         \"w\") as f:\n",
    "                #     f.write(text)\n",
    "                #     print(f\"Extracted text saved to {f.name}.\")\n",
    "                pass\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\") if p.strip() != \"\"]\n",
    "\n",
    "    return paragraphs\n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\"Parse command-line arguments\"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    arguments = [\n",
    "        (\"--input_path\", {\"type\": str,\n",
    "         \"help\": \"input file or folder to translate\"}),\n",
    "        (\"--openai_key\", {\"type\": str,\n",
    "         \"default\": \"\", \"help\": \"OpenAI API key\"}),\n",
    "        (\"--model\", {\"type\": str, \"default\": \"gpt-3.5-turbo\",\n",
    "         \"help\": \"Model to use for translation, e.g., 'gpt-3.5-turbo' or 'gpt-4'\"}),\n",
    "        (\"--num_threads\", {\"type\": int, \"default\": 10,\n",
    "         \"help\": \"number of threads to use for translation\"}),\n",
    "        (\"--target_language\", {\"type\": str, \"default\": \"Simplified Chinese\",\n",
    "         \"help\": \"target language to translate to\"}),\n",
    "        (\"--only_process_this_file_extension\",\n",
    "         {\"type\": str, \"default\": \"\", \"help\": \"only process files with this extension\"}),\n",
    "        (\"--use_azure\", {\"action\": \"store_true\", \"default\": False,\n",
    "         \"help\": \"Use Azure OpenAI service instead of OpenAI platform.\"}),\n",
    "        (\"--azure_endpoint\",\n",
    "         {\"type\": str, \"default\": \"\", \"help\": \"Endpoint URL of Azure OpenAI service. Only require when use AOAI.\"}),\n",
    "        (\"--azure_deployment_name\",\n",
    "         {\"type\": str, \"default\": \"\", \"help\": \"Deployment of Azure OpenAI service. Only require when use AOAI.\"}),\n",
    "    ]\n",
    "\n",
    "    for argument, kwargs in arguments:\n",
    "        parser.add_argument(argument, **kwargs)\n",
    "\n",
    "    options = parser.parse_args()\n",
    "    OPENAI_API_KEY = options.openai_key or token #os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise Exception(\"Please provide your OpenAI API key\")\n",
    "    if options.use_azure:\n",
    "        assert options.azure_endpoint is not None and options.azure_endpoint != '', \"--azure_endpoint is required when use Azure\"\n",
    "        assert options.azure_deployment_name is not None and options.azure_deployment_name, \"--azure_deployment_name is required when use Azure\"\n",
    "    return options\n",
    "\n",
    "\n",
    "def check_file_path(file_path: Path):\n",
    "    \"\"\"\n",
    "    Ensure file extension is in ALLOWED_FILE_TYPES or is a URL.\n",
    "    If file ends with _translated.txt or _bilingual.txt, skip it.\n",
    "    \"\"\"\n",
    "    if not file_path.suffix.lower() in ALLOWED_FILE_TYPES and not str(\n",
    "            file_path).startswith('http'):\n",
    "        print(f\"File extension {file_path.suffix} is not allowed.\")\n",
    "        # raise Exception(\"Please use a txt file or URL\") \n",
    "        # Currently set not to support URL:\n",
    "        raise Exception(\"Please use a txt file\") \n",
    "\n",
    "    if file_path.stem.endswith(\"_translated\") or file_path.stem.endswith(\n",
    "            \"extracted_translated\"):\n",
    "        print(\n",
    "            f\"You already have a translated file for {file_path}, skipping...\")\n",
    "        return False\n",
    "    elif file_path.stem.endswith(\"_bilingual\") or file_path.stem.endswith(\n",
    "            \"extracted_bilingual\"):\n",
    "        print(\n",
    "            f\"You already have a bilingual file for {file_path}, skipping...\")\n",
    "        return False\n",
    "\n",
    "    if (file_path.with_name(f\"{file_path.stem}_translated.txt\").exists() or\n",
    "            file_path.with_name(f\"{file_path.stem}_extracted_translated.txt\").exists()):\n",
    "        print(\n",
    "            f\"You already have a translated file for {file_path}, skipping...\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def process_file(file_path, options):\n",
    "    \"\"\"Translate a single text file\"\"\"\n",
    "    if not check_file_path(file_path):\n",
    "        return\n",
    "    print(f\"Translating {file_path}...\")\n",
    "    translate_text_file(str(file_path), options)\n",
    "\n",
    "\n",
    "def process_folder(folder_path, options):\n",
    "    \"\"\"Translate all text files in a folder\"\"\"\n",
    "    # if only_process_this_file_extension is set, only process files with this extension\n",
    "    if options.only_process_this_file_extension:\n",
    "        files_to_process = list(\n",
    "            folder_path.rglob(f\"*.{options.only_process_this_file_extension}\"))\n",
    "        print(\n",
    "            f\"Only processing files with extension {options.only_process_this_file_extension}\"\n",
    "        )\n",
    "        print(f\"Found {len(files_to_process)} files to process\")\n",
    "    else:\n",
    "        files_to_process = list(folder_path.rglob(\"*\"))\n",
    "    total_files = len(files_to_process)\n",
    "    for index, file_path in enumerate(files_to_process):\n",
    "        if file_path.is_file() and file_path.suffix.lower(\n",
    "        ) in ALLOWED_FILE_TYPES:\n",
    "            process_file(file_path, options)\n",
    "        print(\n",
    "            f\"Processed file {index + 1} of {total_files}. Only {total_files - index - 1} files left to process.\"\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set basic parameters\n",
    "options = {\n",
    "    'target_language': 'English'\n",
    "}\n",
    "file_path = \"/Users/kwang3/Desktop/LLM_translate/pdf_extract_test.txt\"#Argentina_Resolution_45-2022_SP.pdf\"\n",
    "input_path = Path(file_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acutal run the task\n",
    "process_file(input_path, options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
